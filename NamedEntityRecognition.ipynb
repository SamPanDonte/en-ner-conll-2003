{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj6Eu-OqLgmF"
      },
      "source": [
        "# Named entity recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi8YTzccNFtH"
      },
      "source": [
        "## Utilities for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my4F3kaxNQQG"
      },
      "source": [
        "### Get device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "row-eA6uNSey"
      },
      "outputs": [],
      "source": [
        "from torch.cuda import is_available as is_cuda_available\n",
        "from torch.backends.mps import is_available as is_mps_available\n",
        "\n",
        "def get_device() -> str:\n",
        "    '''Returns device string with priority: cuda, mps and cpu'''\n",
        "    if is_cuda_available():\n",
        "        return 'cuda'\n",
        "    if is_mps_available():\n",
        "        return 'mps'\n",
        "    return 'cpu'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkOwovYJjuu4"
      },
      "source": [
        "### Training and evaluating utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6si4QvIvj54t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from time import time\n",
        "from torch.nn import Module\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "def _train_epoch(device: str, model: Module, data: IterableDataset, loss_fn: Module, optimizer: Optimizer):\n",
        "    '''Trains model for one epoch'''\n",
        "    batch_amount = len(data)\n",
        "    begin_time = time()\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    correct = 0\n",
        "    total_amount = 0\n",
        "\n",
        "    for index, (x, y) in enumerate(data, 1):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        out = model(x)\n",
        "\n",
        "        total_amount += torch.count_nonzero(out).item()\n",
        "        for i, j in zip(torch.argmax(out, dim=1), y):\n",
        "            if i == j and i != 0:\n",
        "                correct += 1\n",
        "\n",
        "        loss = loss_fn(out, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        elapsed = time() - begin_time\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        avarage_time = elapsed / index\n",
        "        average_loss = total_loss / index\n",
        "\n",
        "        remaining = avarage_time * (batch_amount - index)\n",
        "\n",
        "        print(f'Train: {index}/{batch_amount} | Eta: {remaining:>0.0f}s', end=' ')\n",
        "        print(f'| loss: {average_loss:>0.4f} - acc: {(correct / total_amount):>0.4f}', end='\\r')\n",
        "\n",
        "        if index == batch_amount:\n",
        "            print()\n",
        "\n",
        "\n",
        "def evaluate_model(device: str, model: Module, data: IterableDataset, loss_fn: Module):\n",
        "    '''Evaluates model with data'''\n",
        "    batch_amount = len(data)\n",
        "    begin_time = time()\n",
        "    total_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total_amount = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for index, (x, y) in enumerate(data, 1):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            out = model(x)\n",
        "\n",
        "            total_amount += torch.count_nonzero(out).item()\n",
        "            for i, j in zip(torch.argmax(out, dim=1), y):\n",
        "                if i == j and i != 0:\n",
        "                    correct += 1\n",
        "\n",
        "            elapsed = time() - begin_time\n",
        "            total_loss += loss_fn(out, y).item()\n",
        "\n",
        "            avarage_time = elapsed / index\n",
        "            average_loss = total_loss / index\n",
        "\n",
        "            remaining = avarage_time * (batch_amount - index)\n",
        "\n",
        "            print(f'Valid {index}/{batch_amount} | Eta: {remaining:>0.0f}s', end=' ')\n",
        "            print(f'| loss: {average_loss:>0.4f} - acc: {(correct / total_amount):>0.4f}', end='\\r')\n",
        "\n",
        "\n",
        "\n",
        "    loss = total_loss / batch_amount\n",
        "    print(f'Valid {batch_amount}/{batch_amount} | Eta: {0}s', end=' ')\n",
        "    print(f'| loss: {loss:>0.4f} - acc: {(correct / total_amount):>0.4f}')\n",
        "\n",
        "\n",
        "def train(device: str, model: Module, data: IterableDataset, valid: IterableDataset, loss_fn: Module, optimizer: Optimizer, epochs: int):\n",
        "    '''Trains model'''\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch + 1}/{epochs}')\n",
        "        _train_epoch(device, model, data, loss_fn, optimizer)\n",
        "        evaluate_model(device, model, valid, loss_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tycrm4UuM5Kw"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vh9HA_JM0tG"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Q-S0-4Ee-Dfo"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/NamedEntityRecognition'\n",
        "TRAIN_DIR = 'train'\n",
        "VALID_DIR = 'dev-0'\n",
        "TEST_DIR = 'test-A'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH1AbKD-QQg2"
      },
      "source": [
        "### Create vocabulary utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1xgU9zTRQULd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab, Vocab\n",
        "\n",
        "TAGS = {\n",
        "    'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5,\n",
        "    'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8\n",
        "}\n",
        "\n",
        "TAGS_REV = {\n",
        "    0: 'O', 1: 'B-ORG', 2: 'I-ORG', 3: 'B-PER', 4: 'I-PER', 5: 'B-LOC',\n",
        "    6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'\n",
        "}\n",
        "\n",
        "def create_vocabulary(data) -> Vocab:\n",
        "    '''Creates vocabulary from dataset'''\n",
        "    counter = Counter()\n",
        "    for text in data:\n",
        "        counter.update(text.split())\n",
        "\n",
        "    vocabulary = vocab(counter, 1, ['<unk>'])\n",
        "    vocabulary.set_default_index(0)\n",
        "\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "def encode(vocab: Vocab, text: str) -> Tensor:\n",
        "    '''Encode word using vocabulary'''\n",
        "    return torch.tensor([vocab[token] for token in text.split()], dtype=torch.long)\n",
        "\n",
        "\n",
        "def encode_tag(text: str) -> Tensor:\n",
        "    '''Encode tag'''\n",
        "    return torch.tensor([TAGS[token] for token in text.split()], dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KufcCQ9GMXUH"
      },
      "source": [
        "### Load and basic prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OAn6IMusLfuU"
      },
      "outputs": [],
      "source": [
        "from os import path\n",
        "\n",
        "\n",
        "def load_data(dir: str, vocab: Vocab) -> tuple[list[Tensor], list[Tensor]]:\n",
        "    '''Loads data from folder'''\n",
        "    labels = []\n",
        "    data = []\n",
        "    with open(path.join(dir, 'in.tsv'), encoding='utf8') as data_file:\n",
        "        for line in data_file:\n",
        "            line = line.split('\\t')\n",
        "            if len(line) == 2:\n",
        "                labels.append(encode_tag(line[0].strip()))\n",
        "                data.append(encode(vocab, line[1]))\n",
        "            else:\n",
        "                data.append(encode(vocab, line[0]))\n",
        "    if path.exists(path.join(dir, 'expected.tsv')):\n",
        "        with open(path.join(dir, 'expected.tsv'), encoding='utf8') as label_file:\n",
        "            labels = [encode_tag(line.strip()) for line in label_file]\n",
        "    return data, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f3L2xKl8lz9"
      },
      "source": [
        "### Custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "q4BKXfiy8lz9"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Iterator\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "\n",
        "class VariableLengthDataset(IterableDataset):\n",
        "    '''Dataset that works with variable length inputs'''\n",
        "    def __init__(self, data: list[Tensor], labels: Tensor) -> None:\n",
        "        self.data = list(filter(lambda x: x[0].size() != torch.Size([0]), zip(data, labels)))\n",
        "\n",
        "    def __getitem__(self, index) -> tuple[Tensor, Any]:\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __iter__(self) -> Iterator[tuple[Tensor, Any]]:\n",
        "        for item in self.data:\n",
        "          yield item\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2OATuCjVtNT"
      },
      "source": [
        "### Loading and encoding data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dRI-Taup8lz9"
      },
      "outputs": [],
      "source": [
        "vocabulary = create_vocabulary(map(lambda x: x.split('\\t')[1], open(path.join(BASE_PATH, TRAIN_DIR, 'in.tsv'), encoding='utf8')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1lT6eNeoVv3p"
      },
      "outputs": [],
      "source": [
        "train_data, train_labels = load_data(path.join(BASE_PATH, TRAIN_DIR), vocabulary)\n",
        "valid_data, valid_labels = load_data(path.join(BASE_PATH, VALID_DIR), vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Ezv_lR4J8lz9"
      },
      "outputs": [],
      "source": [
        "train_data = VariableLengthDataset(train_data, train_labels)\n",
        "valid_data = VariableLengthDataset(valid_data, valid_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHx9pchRY-3b"
      },
      "source": [
        "## Defining model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4aIntHbboJG",
        "outputId": "a0a47090-cd82-4c99-d3bb-a22691f37918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = get_device()\n",
        "print(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IK5SZ14vZTjr"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 64\n",
        "LSTM_DIM = 64\n",
        "LSTM_LAYERS = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8d9pC1bZH0a",
        "outputId": "316be278-3d48-490b-85a1-0e5eff7d7f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (embedding): Embedding(23625, 64, padding_idx=0)\n",
            "  (lstm): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import Linear, Module, ReLU, Embedding, LSTM, Dropout\n",
        "import torch.nn.functional as functional\n",
        "\n",
        "\n",
        "class NeuralNetwork(Module):\n",
        "    def __init__(self, vocab: Vocab):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(len(vocab), EMBEDDING_DIM, padding_idx=0)\n",
        "        self.lstm = LSTM(EMBEDDING_DIM, LSTM_DIM, num_layers=LSTM_LAYERS, batch_first=True, bidirectional=True)\n",
        "        self.linear = Linear(LSTM_DIM * 2, len(TAGS))\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.embedding(input)\n",
        "        x, _ = self.lstm(x.view(len(input), 1, -1))\n",
        "        x = self.linear(x.view(len(input), -1))\n",
        "        logits = functional.log_softmax(x, dim=1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = NeuralNetwork(vocabulary).to(DEVICE)\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjdvNr-ib0HP"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCdHvuavb2II",
        "outputId": "d819c847-b6ed-45af-8384-7554ed96a3d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.7014 - acc: 0.0005\n",
            "Valid 215/215 | Eta: 0s | loss: 0.5521 - acc: 0.0019\n",
            "Epoch 2/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.4631 - acc: 0.0038\n",
            "Valid 215/215 | Eta: 0s | loss: 0.4308 - acc: 0.0049\n",
            "Epoch 3/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.3510 - acc: 0.0068\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3606 - acc: 0.0069\n",
            "Epoch 4/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.2733 - acc: 0.0090\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3163 - acc: 0.0082\n",
            "Epoch 5/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.2177 - acc: 0.0105\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2880 - acc: 0.0091\n",
            "Epoch 6/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.1775 - acc: 0.0117\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2703 - acc: 0.0097\n",
            "Epoch 7/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.1482 - acc: 0.0125\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2599 - acc: 0.0101\n",
            "Epoch 8/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.1270 - acc: 0.0132\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2547 - acc: 0.0104\n",
            "Epoch 9/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.1119 - acc: 0.0136\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2527 - acc: 0.0106\n",
            "Epoch 10/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.1012 - acc: 0.0139\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2525 - acc: 0.0107\n",
            "Epoch 11/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0936 - acc: 0.0141\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2533 - acc: 0.0107\n",
            "Epoch 12/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0883 - acc: 0.0143\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2543 - acc: 0.0108\n",
            "Epoch 13/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0845 - acc: 0.0144\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2559 - acc: 0.0108\n",
            "Epoch 14/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0817 - acc: 0.0144\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2576 - acc: 0.0109\n",
            "Epoch 15/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0797 - acc: 0.0145\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2596 - acc: 0.0109\n",
            "Epoch 16/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0782 - acc: 0.0145\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2612 - acc: 0.0109\n",
            "Epoch 17/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0770 - acc: 0.0146\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2635 - acc: 0.0109\n",
            "Epoch 18/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0761 - acc: 0.0146\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2650 - acc: 0.0109\n",
            "Epoch 19/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0753 - acc: 0.0146\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2672 - acc: 0.0109\n",
            "Epoch 20/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0746 - acc: 0.0146\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2685 - acc: 0.0109\n",
            "Epoch 21/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0741 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2705 - acc: 0.0109\n",
            "Epoch 22/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0736 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2716 - acc: 0.0109\n",
            "Epoch 23/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0732 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2735 - acc: 0.0109\n",
            "Epoch 24/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0729 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2746 - acc: 0.0109\n",
            "Epoch 25/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0725 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2764 - acc: 0.0114\n",
            "Epoch 26/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0723 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2774 - acc: 0.0114\n",
            "Epoch 27/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0720 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2791 - acc: 0.0114\n",
            "Epoch 28/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0718 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2800 - acc: 0.0114\n",
            "Epoch 29/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0715 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2817 - acc: 0.0114\n",
            "Epoch 30/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0713 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2825 - acc: 0.0114\n",
            "Epoch 31/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0711 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2843 - acc: 0.0114\n",
            "Epoch 32/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0710 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2850 - acc: 0.0114\n",
            "Epoch 33/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0708 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2867 - acc: 0.0114\n",
            "Epoch 34/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0706 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2874 - acc: 0.0114\n",
            "Epoch 35/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0705 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2892 - acc: 0.0114\n",
            "Epoch 36/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0704 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2898 - acc: 0.0114\n",
            "Epoch 37/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0702 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2915 - acc: 0.0114\n",
            "Epoch 38/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0701 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2920 - acc: 0.0114\n",
            "Epoch 39/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0700 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2937 - acc: 0.0114\n",
            "Epoch 40/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0698 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2942 - acc: 0.0114\n",
            "Epoch 41/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0697 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2958 - acc: 0.0114\n",
            "Epoch 42/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0696 - acc: 0.0147\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2962 - acc: 0.0114\n",
            "Epoch 43/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0695 - acc: 0.0148\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2979 - acc: 0.0114\n",
            "Epoch 44/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0694 - acc: 0.0148\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2982 - acc: 0.0114\n",
            "Epoch 45/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0693 - acc: 0.0148\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2999 - acc: 0.0114\n",
            "Epoch 46/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0692 - acc: 0.0148\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3003 - acc: 0.0114\n",
            "Epoch 47/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0691 - acc: 0.0148\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3020 - acc: 0.0114\n",
            "Epoch 48/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0691 - acc: 0.0148\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3024 - acc: 0.0114\n",
            "Epoch 49/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0690 - acc: 0.0148\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3042 - acc: 0.0114\n",
            "Epoch 50/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0689 - acc: 0.0148\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3044 - acc: 0.0114\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import NLLLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "loss_fn = NLLLoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "train(DEVICE, model, train_data, valid_data, loss_fn, optimizer, 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IFGoJTU98lz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "affef10b-8629-430b-f6b8-2cfd37a8a22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid 215/215 | Eta: 0s | loss: 0.3044 - acc: 0.0114\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(DEVICE, model, valid_data, loss_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlwYF4W38lz-"
      },
      "source": [
        "## Saving the valid output data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_g9HVhjGhZlI"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_and_save(device: str, model: Module, data: list[Tensor], dir: str):\n",
        "    '''Evaluates model with data and saves results in out.tsv file'''\n",
        "    batch_amount = len(data)\n",
        "    begin_time = time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with open(path.join(dir, 'out.tsv'), 'w') as out_file:\n",
        "        with torch.no_grad():\n",
        "            for index, x in enumerate(data, 1):\n",
        "                x = x.to(device)\n",
        "\n",
        "                out = model(x)\n",
        "                out = torch.argmax(out, dim=1)\n",
        "\n",
        "                for i in out:\n",
        "                    print(TAGS_REV[i.item()], file=out_file, end=' ')\n",
        "                print(file=out_file)\n",
        "\n",
        "                elapsed = time() - begin_time\n",
        "\n",
        "                avarage_time = elapsed / index\n",
        "\n",
        "                remaining = avarage_time * (batch_amount - index)\n",
        "\n",
        "                print(f'Progress {index}/{batch_amount} | Eta: {remaining:>0.0f}s', end='\\r')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fW38XJO18lz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad93b6a0-5769-40d4-b1c5-a03284e0b853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress 215/215 | Eta: 0s\n"
          ]
        }
      ],
      "source": [
        "evaluate_model_and_save(DEVICE, model, [x for (x, _) in valid_data], path.join(BASE_PATH, VALID_DIR))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_z2I2Xzi8lz_"
      },
      "outputs": [],
      "source": [
        "test_data, _ = load_data(path.join(BASE_PATH, TEST_DIR), vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "z7iIugdM8lz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2ae775-2566-4502-c54f-27ef23ca38a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress 230/230 | Eta: 0s\n"
          ]
        }
      ],
      "source": [
        "evaluate_model_and_save(DEVICE, model, [x for x in test_data], path.join(BASE_PATH, TEST_DIR))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}