{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj6Eu-OqLgmF"
      },
      "source": [
        "# Named entity recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi8YTzccNFtH"
      },
      "source": [
        "## Utilities for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my4F3kaxNQQG"
      },
      "source": [
        "### Get device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "row-eA6uNSey"
      },
      "outputs": [],
      "source": [
        "from torch.cuda import is_available as is_cuda_available\n",
        "from torch.backends.mps import is_available as is_mps_available\n",
        "\n",
        "def get_device() -> str:\n",
        "    '''Returns device string with priority: cuda, mps and cpu'''\n",
        "    if is_cuda_available():\n",
        "        return 'cuda'\n",
        "    if is_mps_available():\n",
        "        return 'mps'\n",
        "    return 'cpu'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkOwovYJjuu4"
      },
      "source": [
        "### Training and evaluating utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6si4QvIvj54t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from time import time\n",
        "from torch.nn import Module\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "def _train_epoch(device: str, model: Module, data: IterableDataset, loss_fn: Module, optimizer: Optimizer):\n",
        "    '''Trains model for one epoch'''\n",
        "    batch_amount = len(data)\n",
        "    begin_time = time()\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    correct = 0\n",
        "    total_amount = 0\n",
        "\n",
        "    for index, (x, y) in enumerate(data, 1):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        out = model(x)\n",
        "\n",
        "        total_amount += torch.count_nonzero(y).item()\n",
        "        for i, j in zip(torch.argmax(out, dim=1), y):\n",
        "            if i == j and i != 0:\n",
        "                correct += 1\n",
        "\n",
        "        loss = loss_fn(out, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        elapsed = time() - begin_time\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        avarage_time = elapsed / index\n",
        "        average_loss = total_loss / index\n",
        "\n",
        "        remaining = avarage_time * (batch_amount - index)\n",
        "\n",
        "        print(f'Train: {index}/{batch_amount} | Eta: {remaining:>0.0f}s', end=' ')\n",
        "        print(f'| loss: {average_loss:>0.4f} - acc: {(correct / total_amount):>0.4f}', end='\\r')\n",
        "\n",
        "        if index == batch_amount:\n",
        "            print()\n",
        "\n",
        "\n",
        "def evaluate_model(device: str, model: Module, data: IterableDataset, loss_fn: Module):\n",
        "    '''Evaluates model with data'''\n",
        "    batch_amount = len(data)\n",
        "    begin_time = time()\n",
        "    total_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total_amount = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for index, (x, y) in enumerate(data, 1):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            out = model(x)\n",
        "\n",
        "            total_amount += torch.count_nonzero(y).item()\n",
        "            for i, j in zip(torch.argmax(out, dim=1), y):\n",
        "                if i == j and i != 0:\n",
        "                    correct += 1\n",
        "\n",
        "            elapsed = time() - begin_time\n",
        "            total_loss += loss_fn(out, y).item()\n",
        "\n",
        "            avarage_time = elapsed / index\n",
        "            average_loss = total_loss / index\n",
        "\n",
        "            remaining = avarage_time * (batch_amount - index)\n",
        "\n",
        "            print(f'Valid {index}/{batch_amount} | Eta: {remaining:>0.0f}s', end=' ')\n",
        "            print(f'| loss: {average_loss:>0.4f} - acc: {(correct / total_amount):>0.4f}', end='\\r')\n",
        "\n",
        "\n",
        "\n",
        "    loss = total_loss / batch_amount\n",
        "    print(f'Valid {batch_amount}/{batch_amount} | Eta: {0}s', end=' ')\n",
        "    print(f'| loss: {loss:>0.4f} - acc: {(correct / total_amount):>0.4f}')\n",
        "\n",
        "\n",
        "def train(device: str, model: Module, data: IterableDataset, valid: IterableDataset, loss_fn: Module, optimizer: Optimizer, epochs: int):\n",
        "    '''Trains model'''\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch + 1}/{epochs}')\n",
        "        _train_epoch(device, model, data, loss_fn, optimizer)\n",
        "        evaluate_model(device, model, valid, loss_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tycrm4UuM5Kw"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vh9HA_JM0tG"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q-S0-4Ee-Dfo"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/NamedEntityRecognition'\n",
        "TRAIN_DIR = 'train'\n",
        "VALID_DIR = 'dev-0'\n",
        "TEST_DIR = 'test-A'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH1AbKD-QQg2"
      },
      "source": [
        "### Create vocabulary utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1xgU9zTRQULd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab, Vocab\n",
        "\n",
        "TAGS = {\n",
        "    'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5,\n",
        "    'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8\n",
        "}\n",
        "\n",
        "TAGS_REV = {\n",
        "    0: 'O', 1: 'B-ORG', 2: 'I-ORG', 3: 'B-PER', 4: 'I-PER', 5: 'B-LOC',\n",
        "    6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'\n",
        "}\n",
        "\n",
        "def create_vocabulary(data) -> Vocab:\n",
        "    '''Creates vocabulary from dataset'''\n",
        "    counter = Counter()\n",
        "    for text in data:\n",
        "        counter.update(text.split())\n",
        "\n",
        "    vocabulary = vocab(counter, 1, ['<unk>'])\n",
        "    vocabulary.set_default_index(0)\n",
        "\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "def encode(vocab: Vocab, text: str) -> Tensor:\n",
        "    '''Encode word using vocabulary'''\n",
        "    return torch.tensor([vocab[token] for token in text.split()], dtype=torch.long)\n",
        "\n",
        "\n",
        "def encode_tag(text: str) -> Tensor:\n",
        "    '''Encode tag'''\n",
        "    return torch.tensor([TAGS[token] for token in text.split()], dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KufcCQ9GMXUH"
      },
      "source": [
        "### Load and basic prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OAn6IMusLfuU"
      },
      "outputs": [],
      "source": [
        "from os import path\n",
        "\n",
        "\n",
        "def load_data(dir: str, vocab: Vocab) -> tuple[list[Tensor], list[Tensor]]:\n",
        "    '''Loads data from folder'''\n",
        "    labels = []\n",
        "    data = []\n",
        "    with open(path.join(dir, 'in.tsv'), encoding='utf8') as data_file:\n",
        "        for line in data_file:\n",
        "            line = line.split('\\t')\n",
        "            if len(line) == 2:\n",
        "                labels.append(encode_tag(line[0].strip()))\n",
        "                data.append(encode(vocab, line[1]))\n",
        "            else:\n",
        "                data.append(encode(vocab, line[0]))\n",
        "    if path.exists(path.join(dir, 'expected.tsv')):\n",
        "        with open(path.join(dir, 'expected.tsv'), encoding='utf8') as label_file:\n",
        "            labels = [encode_tag(line.strip()) for line in label_file]\n",
        "    return data, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f3L2xKl8lz9"
      },
      "source": [
        "### Custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q4BKXfiy8lz9"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Iterator\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "\n",
        "class VariableLengthDataset(IterableDataset):\n",
        "    '''Dataset that works with variable length inputs'''\n",
        "    def __init__(self, data: list[Tensor], labels: Tensor) -> None:\n",
        "        self.data = list(filter(lambda x: x[0].size() != torch.Size([0]), zip(data, labels)))\n",
        "\n",
        "    def __getitem__(self, index) -> tuple[Tensor, Any]:\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __iter__(self) -> Iterator[tuple[Tensor, Any]]:\n",
        "        for item in self.data:\n",
        "          yield item\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2OATuCjVtNT"
      },
      "source": [
        "### Loading and encoding data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dRI-Taup8lz9"
      },
      "outputs": [],
      "source": [
        "vocabulary = create_vocabulary(map(lambda x: x.split('\\t')[1], open(path.join(BASE_PATH, TRAIN_DIR, 'in.tsv'), encoding='utf8')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1lT6eNeoVv3p"
      },
      "outputs": [],
      "source": [
        "train_data, train_labels = load_data(path.join(BASE_PATH, TRAIN_DIR), vocabulary)\n",
        "valid_data, valid_labels = load_data(path.join(BASE_PATH, VALID_DIR), vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ezv_lR4J8lz9"
      },
      "outputs": [],
      "source": [
        "train_data = VariableLengthDataset(train_data, train_labels)\n",
        "valid_data = VariableLengthDataset(valid_data, valid_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHx9pchRY-3b"
      },
      "source": [
        "## Defining model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4aIntHbboJG",
        "outputId": "e186278e-86fe-4b0d-ca58-6fd2da9f6c6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = get_device()\n",
        "print(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IK5SZ14vZTjr"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 256\n",
        "LSTM_DIM = 256\n",
        "LSTM_LAYERS = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8d9pC1bZH0a",
        "outputId": "4e3f2f34-ea8e-4cf2-8434-2e3b438e60a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (embedding): Embedding(23625, 256, padding_idx=0)\n",
            "  (lstm): LSTM(256, 256, num_layers=5, batch_first=True, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (linear): Linear(in_features=512, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import Linear, Module, ReLU, Embedding, LSTM, Dropout\n",
        "import torch.nn.functional as functional\n",
        "\n",
        "\n",
        "class NeuralNetwork(Module):\n",
        "    def __init__(self, vocab: Vocab):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(len(vocab), EMBEDDING_DIM, padding_idx=0)\n",
        "        self.lstm = LSTM(EMBEDDING_DIM, LSTM_DIM, num_layers=LSTM_LAYERS, batch_first=True, bidirectional=True)\n",
        "        self.dropout = Dropout(0.3)\n",
        "        self.linear = Linear(LSTM_DIM * 2, len(TAGS))\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.embedding(input)\n",
        "        x, _ = self.lstm(x.view(len(input), 1, -1))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x.view(len(input), -1))\n",
        "        logits = functional.log_softmax(x, dim=1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = NeuralNetwork(vocabulary).to(DEVICE)\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjdvNr-ib0HP"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCdHvuavb2II",
        "outputId": "32daf278-285a-429b-d201-042915f1077f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.5678 - acc: 0.0948\n",
            "Valid 215/215 | Eta: 0s | loss: 0.4944 - acc: 0.1907\n",
            "Epoch 2/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.3578 - acc: 0.3415\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3819 - acc: 0.3548\n",
            "Epoch 3/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.2719 - acc: 0.4926\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3294 - acc: 0.4521\n",
            "Epoch 4/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.2127 - acc: 0.5921\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2984 - acc: 0.5050\n",
            "Epoch 5/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.1703 - acc: 0.6663\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2790 - acc: 0.5396\n",
            "Epoch 6/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.1469 - acc: 0.7102\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2751 - acc: 0.5570\n",
            "Epoch 7/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.1266 - acc: 0.7487\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2658 - acc: 0.5784\n",
            "Epoch 8/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.1132 - acc: 0.7756\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2962 - acc: 0.5832\n",
            "Epoch 9/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.1079 - acc: 0.7879\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3104 - acc: 0.5965\n",
            "Epoch 10/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0990 - acc: 0.8041\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2818 - acc: 0.6050\n",
            "Epoch 11/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0942 - acc: 0.8103\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3040 - acc: 0.6049\n",
            "Epoch 12/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0911 - acc: 0.8159\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3502 - acc: 0.6024\n",
            "Epoch 13/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0871 - acc: 0.8211\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3096 - acc: 0.6082\n",
            "Epoch 14/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0831 - acc: 0.8270\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3277 - acc: 0.6063\n",
            "Epoch 15/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0831 - acc: 0.8281\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3129 - acc: 0.6031\n",
            "Epoch 16/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0802 - acc: 0.8304\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3225 - acc: 0.6070\n",
            "Epoch 17/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0774 - acc: 0.8323\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3228 - acc: 0.6099\n",
            "Epoch 18/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0774 - acc: 0.8335\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3535 - acc: 0.6094\n",
            "Epoch 19/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0815 - acc: 0.8270\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3005 - acc: 0.6109\n",
            "Epoch 20/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0769 - acc: 0.8351\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3043 - acc: 0.6101\n",
            "Epoch 21/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0743 - acc: 0.8371\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3190 - acc: 0.6087\n",
            "Epoch 22/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0738 - acc: 0.8378\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3586 - acc: 0.6715\n",
            "Epoch 23/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0742 - acc: 0.8371\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3075 - acc: 0.6056\n",
            "Epoch 24/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0796 - acc: 0.8303\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2927 - acc: 0.6060\n",
            "Epoch 25/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0738 - acc: 0.8380\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3294 - acc: 0.6622\n",
            "Epoch 26/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0720 - acc: 0.8405\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3674 - acc: 0.6614\n",
            "Epoch 27/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0719 - acc: 0.8407\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3502 - acc: 0.6643\n",
            "Epoch 28/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0725 - acc: 0.8397\n",
            "Valid 215/215 | Eta: 0s | loss: 0.4025 - acc: 0.6621\n",
            "Epoch 29/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0719 - acc: 0.8391\n",
            "Valid 215/215 | Eta: 0s | loss: 0.4272 - acc: 0.6733\n",
            "Epoch 30/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0744 - acc: 0.8366\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2954 - acc: 0.6048\n",
            "Epoch 31/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0772 - acc: 0.8311\n",
            "Valid 215/215 | Eta: 0s | loss: 0.2957 - acc: 0.6065\n",
            "Epoch 32/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0715 - acc: 0.8419\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3285 - acc: 0.6593\n",
            "Epoch 33/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0705 - acc: 0.8424\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3266 - acc: 0.6646\n",
            "Epoch 34/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0703 - acc: 0.8414\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3358 - acc: 0.6683\n",
            "Epoch 35/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0705 - acc: 0.8407\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3716 - acc: 0.6697\n",
            "Epoch 36/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0711 - acc: 0.8408\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3018 - acc: 0.6134\n",
            "Epoch 37/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0718 - acc: 0.8397\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3240 - acc: 0.6695\n",
            "Epoch 38/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0710 - acc: 0.8401\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3178 - acc: 0.6694\n",
            "Epoch 39/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0714 - acc: 0.8402\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3221 - acc: 0.6686\n",
            "Epoch 40/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0705 - acc: 0.8423\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3714 - acc: 0.6704\n",
            "Epoch 41/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0697 - acc: 0.8406\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3624 - acc: 0.6715\n",
            "Epoch 42/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0695 - acc: 0.8425\n",
            "Valid 215/215 | Eta: 0s | loss: 0.4079 - acc: 0.6671\n",
            "Epoch 43/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0690 - acc: 0.8424\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3945 - acc: 0.6665\n",
            "Epoch 44/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0695 - acc: 0.8422\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3966 - acc: 0.6667\n",
            "Epoch 45/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0700 - acc: 0.8417\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3267 - acc: 0.6681\n",
            "Epoch 46/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0704 - acc: 0.8415\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3272 - acc: 0.6688\n",
            "Epoch 47/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0694 - acc: 0.8429\n",
            "Valid 215/215 | Eta: 0s | loss: 0.3921 - acc: 0.6792\n",
            "Epoch 48/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0691 - acc: 0.8432\n",
            "Valid 215/215 | Eta: 0s | loss: 0.4229 - acc: 0.6681\n",
            "Epoch 49/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0687 - acc: 0.8433\n",
            "Valid 215/215 | Eta: 0s | loss: 0.4410 - acc: 0.6800\n",
            "Epoch 50/50\n",
            "Train: 945/945 | Eta: 0s | loss: 0.0690 - acc: 0.8425\n",
            "Valid 215/215 | Eta: 0s | loss: 0.4206 - acc: 0.6771\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import NLLLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "loss_fn = NLLLoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "train(DEVICE, model, train_data, valid_data, loss_fn, optimizer, 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IFGoJTU98lz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac70dcad-77ce-485e-c517-451f349f3600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid 215/215 | Eta: 0s | loss: 0.4206 - acc: 0.6771\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(DEVICE, model, valid_data, loss_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlwYF4W38lz-"
      },
      "source": [
        "## Saving the valid output data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_g9HVhjGhZlI"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_and_save(device: str, model: Module, data: list[Tensor], dir: str):\n",
        "    '''Evaluates model with data and saves results in out.tsv file'''\n",
        "    batch_amount = len(data)\n",
        "    begin_time = time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with open(path.join(dir, 'out.tsv'), 'w') as out_file:\n",
        "        with torch.no_grad():\n",
        "            for index, x in enumerate(data, 1):\n",
        "                x = x.to(device)\n",
        "\n",
        "                out = functional.softmax(model(x), dim=1)\n",
        "                label = torch.argmax(out, dim=1)\n",
        "\n",
        "                last_label = None\n",
        "                for j, i in enumerate(label):\n",
        "                    if j != 0:\n",
        "                        print(end=' ', file=out_file)\n",
        "                    value = TAGS_REV[i.item()]\n",
        "                    if value != \"O\" and value[0:2] == \"I-\":\n",
        "                        if last_label is None or last_label == \"O\":\n",
        "                            value = value.replace('I-', 'B-')\n",
        "                        else:\n",
        "                            value = \"I-\" + last_label[2:]\n",
        "                    last_label = value\n",
        "                    print('{}'.format(value), file=out_file, end='')\n",
        "                print(file=out_file)\n",
        "\n",
        "                elapsed = time() - begin_time\n",
        "\n",
        "                avarage_time = elapsed / index\n",
        "\n",
        "                remaining = avarage_time * (batch_amount - index)\n",
        "\n",
        "                print(f'Progress {index}/{batch_amount} | Eta: {remaining:>0.0f}s', end='\\r')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fW38XJO18lz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c72777-93ca-4f7b-a3a2-60560b156cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress 215/215 | Eta: 0s\n"
          ]
        }
      ],
      "source": [
        "evaluate_model_and_save(DEVICE, model, [x for (x, _) in valid_data], path.join(BASE_PATH, VALID_DIR))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_z2I2Xzi8lz_"
      },
      "outputs": [],
      "source": [
        "test_data, _ = load_data(path.join(BASE_PATH, TEST_DIR), vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "z7iIugdM8lz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fb6f1c-09e8-45cb-ce5a-0636cec1403a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress 230/230 | Eta: 0s\n"
          ]
        }
      ],
      "source": [
        "evaluate_model_and_save(DEVICE, model, [x for x in test_data], path.join(BASE_PATH, TEST_DIR))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}